{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-08-25-Fibonacci-rescue.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPi4iUqde7WNouoj7yVsTho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalRyszardWojcik/hello-world/blob/master/2020_08_25_Fibonacci_rescue.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJKbIqfX1h56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "! pip install -q -U trax\n",
        "import trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_stPDTB1ler",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = trax.models.TransformerLM(\n",
        "    d_model=32, d_ff=128, n_layers=2, \n",
        "    vocab_size=32, mode='predict')\n",
        "model2 = trax.models.TransformerLM(d_model=1, mode='predict',vocab_size=32,n_heads=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwhZUANC35D6",
        "colab_type": "text"
      },
      "source": [
        "trax.models.TransformerLM(vocab_size, d_model=512, d_ff=2048, n_layers=6, n_heads=8, dropout=0.1, dropout_shared_axes=None, max_len=2048, mode='train', ff_activation=<function Relu at 0x7f553bdde2f0>)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-hUkgxc1w_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2bHcbnvMZVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQL1acEEMfL2",
        "colab_type": "text"
      },
      "source": [
        "I compared the output of model1 with the output of model2 using https://text-compare.com/\n",
        "\n",
        "This gives me an important insight into the complicated structure of a TransformerLM.\n",
        "\n",
        "Another crucial insight comes from reading:\n",
        "<code>Args:\n",
        "    model: A layer object (subclass of `trax.layers.Layer`) created in\n",
        "        `'predict'` mode and initialized from trained weights. The model\n",
        "        must have a structure that allows it to run as an autoregressive\n",
        "        one-sample-at-a-time predictor (e.g., `trax.models.TransformerLM`).</code> from https://github.com/google/trax/blob/master/trax/supervised/decoding.py\n",
        "\n",
        "I plan to train a TransformerLM to generate Fibanacci sequences before I go on with the chess games.\n",
        "\n",
        "The question is how to generate the batches for training. I should generate a tuple (inputs, targets, loss_weights). Should the targets be the same as inputs = the games followed by a game-over token? Should the loss_weights be all ones?\n",
        "\n",
        "It is okay to treat TransformerLM as a blackbox configured with its args but I must understand how to prepare input for training, even if I don't understand the training process mathematically.\n",
        "\n"
      ]
    }
  ]
}